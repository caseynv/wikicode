{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "958403c3-20b8-4599-a0c7-8ad7d85335b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pywikibot\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "import re\n",
    "import ast\n",
    "import json\n",
    "# Connect to enwiki\n",
    "enwiki = pywikibot.Site('en', 'wikipedia')\n",
    "# and then to wikidata\n",
    "enwiki_repo = enwiki.data_repository()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00f9a473-dae9-4a6e-aeec-fadf3f02b1b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['J. A., Rubiño-Martín',\n",
       " 'R., Rebolo',\n",
       " 'M., Aguiar',\n",
       " 'R., Génova-Santos',\n",
       " 'F., Gómez-Reñasco',\n",
       " 'J. M., Herreros',\n",
       " 'R. J., Hoyland',\n",
       " 'C., López-Caraballo',\n",
       " 'A. E., Pelaez Santos',\n",
       " 'V., Sanchez de la Rosa',\n",
       " 'A., Vega-Moreno',\n",
       " 'T., Viera-Curbelo',\n",
       " 'E., Martínez-Gonzalez',\n",
       " 'R. B., Barreiro',\n",
       " 'F. J., Casas',\n",
       " 'J. M., Diego',\n",
       " 'R., Fernández-Cobos',\n",
       " 'D., Herranz',\n",
       " 'M., López-Caniego',\n",
       " 'D., Ortiz',\n",
       " 'P., Vielva',\n",
       " 'E., Artal',\n",
       " 'B., Aja',\n",
       " 'J., Cagigas',\n",
       " 'J. L., Cano',\n",
       " 'L., de la Fuente',\n",
       " 'A., Mediavilla',\n",
       " 'J. V., Terán',\n",
       " 'E., Villa',\n",
       " 'L., Piccirillo',\n",
       " 'R., Battye',\n",
       " 'E., Blackhurst',\n",
       " 'M., Brown',\n",
       " 'R. D., Davies',\n",
       " 'R. J., Davis',\n",
       " 'C., Dickinson',\n",
       " 'S., Harper',\n",
       " 'B., Maffei',\n",
       " 'M., McCulloch',\n",
       " 'S., Melhuish',\n",
       " 'G., Pisano',\n",
       " 'R. A., Watson',\n",
       " 'M., Hobson',\n",
       " 'K., Grainge',\n",
       " 'A., Lasenby',\n",
       " 'R., Saunders',\n",
       " 'P., Scott']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This function extracts author names from crossref \n",
    "\n",
    "def author_citation1(url):\n",
    "    list1 = []\n",
    "    with urllib.request.urlopen(url) as response:\n",
    "        html = response.read()\n",
    "        str1 = html.decode(\"UTF-8\")\n",
    "        useful = json.loads(str1)\n",
    "        name = useful['message']['author']\n",
    "        for i in name:\n",
    "            names = i['given'] + ', ' + i['family']\n",
    "            list1.append(names)\n",
    "    return list1\n",
    "author_citation1('https://api.crossref.org/v1/works/10.1117/12.926581')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3148f9c-4d24-4faa-9c3d-a526b27cebfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R I Richards\n"
     ]
    }
   ],
   "source": [
    "#This function extracts author names from pubmed \n",
    "\n",
    "def author_citation2(url):\n",
    "    list1 = []\n",
    "    with urllib.request.urlopen(url) as response:\n",
    "        html = response.read()\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        text3 = soup.findAll(attrs={'name' : re.compile('^citation_author$')})\n",
    "        for text in text3:\n",
    "            tip = text['content']\n",
    "            #list1.append(tip)\n",
    "            print(tip)\n",
    "author_citation2('https://pubmed.ncbi.nlm.nih.gov/11377796/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d51b6cd-6dc7-4f04-83c3-1cc199c6cd32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mauro Mandrioli', 'Gian Carlo Manicardi']\n"
     ]
    }
   ],
   "source": [
    "#This function extracts author names from PubMed Central \n",
    "\n",
    "def author_citation3(url):\n",
    "    #list1 = []\n",
    "    with urllib.request.urlopen(url) as response:\n",
    "        html = response.read()\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        text3 = soup.findAll(attrs={'name' : re.compile('^citation_authors$')})\n",
    "        for text in text3:\n",
    "            tip = text['content']\n",
    "            #list1.append(tip)\n",
    "            x1 = re.split(\", \", tip)\n",
    "            print(x1)\n",
    "author_citation3('https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7392213/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13c7e16a-2c0b-47e6-9b9f-b3e9f38da68a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Story, K. T.',\n",
       " 'Reichardt, C. L.',\n",
       " 'Hou, Z.',\n",
       " 'Keisler, R.',\n",
       " 'Aird, K. A.',\n",
       " 'Benson, B. A.',\n",
       " 'Bleem, L. E.',\n",
       " 'Carlstrom, J. E.',\n",
       " 'Chang, C. L.',\n",
       " 'Cho, H-M.',\n",
       " 'Crawford, T. M.',\n",
       " 'Crites, A. T.',\n",
       " 'de Haan, T.',\n",
       " 'Dobbs, M. A.',\n",
       " 'Dudley, J.',\n",
       " 'Follin, B.',\n",
       " 'George, E. M.',\n",
       " 'Halverson, N. W.',\n",
       " 'Holder, G. P.',\n",
       " 'Holzapfel, W. L.',\n",
       " 'Hoover, S.',\n",
       " 'Hrubes, J. D.',\n",
       " 'Joy, M.',\n",
       " 'Knox, L.',\n",
       " 'Lee, A. T.',\n",
       " 'Leitch, E. M.',\n",
       " 'Lueker, M.',\n",
       " 'Luong-Van, D.',\n",
       " 'McMahon, J. J.',\n",
       " 'Mehl, J.',\n",
       " 'Meyer, S. S.',\n",
       " 'Millea, M.',\n",
       " 'Mohr, J. J.',\n",
       " 'Montroy, T. E.',\n",
       " 'Padin, S.',\n",
       " 'Plagge, T.',\n",
       " 'Pryke, C.',\n",
       " 'Ruhl, J. E.',\n",
       " 'Sayre, J. T.',\n",
       " 'Schaffer, K. K.',\n",
       " 'Shaw, L.',\n",
       " 'Shirokoff, E.',\n",
       " 'Spieler, H. G.',\n",
       " 'Staniszewski, Z.',\n",
       " 'Stark, A. A.',\n",
       " 'van Engelen, A.',\n",
       " 'Vanderlinde, K.',\n",
       " 'Vieira, J. D.',\n",
       " 'Williamson, R.',\n",
       " 'Zahn, O.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This function extracts author names from arXiv  \n",
    "\n",
    "def author_citation4(url):\n",
    "    list1 = []\n",
    "    with urllib.request.urlopen(url) as response:\n",
    "        html = response.read()\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        text3 = soup.findAll(attrs={'name' : re.compile('^citation_author$')})\n",
    "        for text in text3:\n",
    "            tip = text['content']\n",
    "            list1.append(tip)\n",
    "    return list1\n",
    "author_citation4('https://arxiv.org/abs/1210.7231')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba89a5ab-432d-4c75-b9cc-d6ed524766ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91f98ca-0354-4e6c-bc53-d87307b8285d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This can may be applicable for articles of a particular subject\n",
    "\n",
    "def useurl(list_item):\n",
    "    site = pywikibot.Site(\"wikidata\", \"wikidata\")\n",
    "    repo = site.data_repository()\n",
    "    for data_item in list_item:\n",
    "        item = pywikibot.ItemPage(repo, data_item) \n",
    "        \n",
    "        for url in item.claims:\n",
    "            if 'P819' in url:\n",
    "                source = item.claims[url]\n",
    "                ID = source[0].target\n",
    "                url = 'https://ui.adsabs.harvard.edu/abs/' + ID + '/exportcitation'\n",
    "                return url\n",
    "                author_citation5(url)\n",
    "\n",
    "            elif 'P818' in url:\n",
    "                source1 = item.claims[url]\n",
    "                ID = source1[0].target\n",
    "                url = 'https://arxiv.org/abs/' + ID \n",
    "                return url\n",
    "                author_citation4(url)\n",
    "                    \n",
    "                \n",
    "            elif 'P932' in url:\n",
    "                source1 = item.claims[url]\n",
    "                ID = source1[0].target\n",
    "                url = 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC' + ID #this is dependent on the topic, the parts of the url is needed 698\n",
    "                return url\n",
    "                author_citation2(url)\n",
    "\n",
    "            elif 'P698' in url:\n",
    "                source1 = item.claims[url]\n",
    "                ID = source1[0].target\n",
    "                url = 'https://pubmed.ncbi.nlm.nih.gov/' + ID \n",
    "                return url\n",
    "                author_citation3(url)\n",
    "                   \n",
    "            elif 'P356' in url:\n",
    "                source1 = item.claims[url]\n",
    "                ID = source1[0].target\n",
    "                url = 'https://api.crossref.org/v1/works/' + ID #this is a third-party formatter calles crossref, read more here: https://www.wikidata.org/wiki/Property:P356\n",
    "                return url\n",
    "                author_citation1(url)\n",
    "                \n",
    "            else:\n",
    "                continue\n",
    "        \n",
    "useurl(['Q60868120'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
